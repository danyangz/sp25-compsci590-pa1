{"cells":[{"cell_type":"markdown","metadata":{"id":"CPrCLeqDcia_"},"source":["# Systems for ML -- Programming Assignment 1\n","\n","<a target=\"_blank\" href=\"https://colab.research.google.com/github/danyangz/sp25-compsci590-pa1/blob/main/mlsys_hw1.ipynb\">\n","  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n","</a>\n","\n","**Homework due: Jan 29, 2025, 11:59 pm, EST**.\n","\n","Automatic differentiation forms the core technique for training machine learning models. In this assignment, you are required to develop a basic automatic differentiation system from scratch. Additionally, you will construct a logistic regression model and apply it to a dataset of handwritten digits to train it.\n","\n","* This assignment must be completed **individually** and is not intended for group work.\n","* No GPU is needed for this assignment. You may choose to work in Google Colab (using the link provided above), on your personal computer, or on any other accessible server.\n","* This assignment solely requires Python programming; no C++ coding is involved.\n","* Make sure to refer to the final section of this notebook for details on how to submit your assignment.\n","* Avoid posting your completed work on any public platforms (such as GitHub).\n","* **Regarding testing and grading:** We have provided a suite of public test scripts located in the `tests/` directory that you can use to verify your implementation. You are allowed to submit your assignment multiple times, but only the last submission will be considered for grading after the deadline.\n"]},{"cell_type":"markdown","metadata":{"id":"NDVuWaftK_h3"},"source":["## Set up\n","\n","* If you decide to use the Google Colab environment for this assignment, start by creating a copy of this notebook. You can do this by choosing \"Save a copy in Drive\" from the \"File\" menu. After saving the copy, execute the code block below to prepare your workspace. Once the repository is cloned, you will be able to view it in the \"Files\" tab on the left side of the screen.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2864,"status":"ok","timestamp":1706413992841,"user":{"displayName":"Ruihang Lai","userId":"03605576729730874720"},"user_tz":300},"id":"hx2QcOuFR6Nu","outputId":"dd2dccc8-63fe-424b-c2cd-5077321470a2"},"outputs":[],"source":["# Code to set up the assignment\n","from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/\n","!mkdir -p cs590\n","%cd /content/drive/MyDrive/cs590\n","!git clone https://github.com/danyangz/sp25-compsci590-pa1.git\n","%cd /content/drive/MyDrive/cs590/sp25-compsci590-pa1"]},{"cell_type":"markdown","metadata":{"id":"ITXgDFZSK_h5"},"source":["* If you are using local/server environment, please clone this repository.\n","\n","```shell\n","git clone https://github.com/danyangz/sp25-compsci590-pa1.git\n","cd sp25-compsci590-pa1\n","export PYTHONPATH=.:$PYTHONPATH\n","```"]},{"cell_type":"markdown","metadata":{"id":"2eWqMr7QK_h5"},"source":["## Part 1: Automatic Differentiation Framework\n","\n","In part 1, you are tasked with coding the reverse mode of the automatic differentiation algorithm.\n","\n","The automatic differentiation algorithm in this assignment operates using a **computational graph**. A computational graph visually represents the sequence of operations needed to compute an expression. For instance, consider the expression $y = x_1 \\times x_2 + x_1$\n","\n","\n","<img src=\"https://github.com/hao-ai-lab/dsc291-PA/blob/main/pa1/figure/computational_graph.jpg?raw=true\" alt=\"figure/computational_graph.jpg\" width=\"60%\"/>"]},{"cell_type":"markdown","metadata":{},"source":["Let's begin by exploring the fundamental concepts and data structures used in the framework.\n","A computational graph is composed of **nodes**, each representing a distinct computation step in the evaluation of the entire expression.\n","Each node consists of three components, as shown in `auto_diff.py` line 6:\n","\n","- an **operation** (field `op`), specifying the type of computation the node performs.\n","- a list of **input nodes** (field `inputs`), detailing the sources of input for the computation.\n","- optionally, additional \"**attributes**\" (field `attrs`), which vary depending on the node's operation. These attributes will be discussed in more detail later in this section.\n","\n","Input nodes in a computational graph can be defined using `ad.Variable`. For instance, the input variable nodes $x_1$ and $x_2$ might be set up as follows:\n","\n","```python\n","import auto_diff as ad\n","\n","x1 = ad.Variable(name=\"x1\")\n","x2 = ad.Variable(name=\"x2\")\n","```\n","\n","In `auto_diff.py` (line 81), the `ad.Variable` class is used to create a node\n","with the operation placeholder and a specified name. Input nodes have empty inputs and attrs:\n","\n","```python\n","class Variable(Node):\n","    def __init__(self, name: str) -> None:\n","        super().__init__(inputs=[], op=placeholder, name=name)\n","```\n","\n","Here, the placeholder operation signifies that the input variable node does not perform any computation. Apart from placeholder, there are other operations defined in auto_diff.py, such as:\n","\n","- `add`, which adds two nodes,\n","- `matmul`, which performs matrix multiplication between two nodes.\n","\n","It is important to note that these operations are globally defined once, and the op field of every node corresponds to one of these globally defined operations. You should not create your own instances of these `ops`."]},{"cell_type":"markdown","metadata":{},"source":["Returning to our example where $y = x_1 \\times x_2 + x_1$, with `x1` and `x2` already established as input variables, the rest of the graph can be defined using just one line of Python:\n","```python\n","y = x1 * x2 + x1\n","```\n","\n","This code first creates a node with the operation `mul`, taking `x1` and `x2` as its inputs. It then constructs another node with `add`, which utilizes the result of the multiplication node along with `x1` as inputs. Consequently, this computational graph ultimately comprises four nodes."]},{"cell_type":"markdown","metadata":{"id":"yYS8MfifK_h6"},"source":["#### Important Note\n","\n","It's important to note that a computational graph (e.g., the four nodes we defined) **does not** inherently store the actual values of its nodes. The structure of this assignment aligns with the TensorFlow v1 approach that was covered in our lectures. This method contrasts with frameworks like PyTorch, where input tensor values are specified upfront, and the values of intermediate tensors are computed immediately as they are defined.\n","\n","In our framework, to calculate the value of the output `y` given the inputs `x1` and `x2`, we utilize the `Evaluator` class found in `auto_diff.py` at line 373.\n"]},{"cell_type":"markdown","metadata":{"id":"lCip5YB7K_h6"},"source":["### Evaluator\n","Here's a walkthrough of how `Evaluator` works. The constructor of `Evaluator` accepts a list of nodes that it needs to evaluate. By initiating an `Evaluator` with:\n","```python\n","evaluator = ad.Evaluator(eval_nodes=[y])\n","```\n","you are essentially setting up an Evaluator instance designed to compute the value of y. To calculate this, input tensor values are provided via the Evaluator.run method, which you will implement. These input tensors are assumed to be of type `numpy.ndarray` throughout this assignment. Here’s how it works:\n","```python\n","import numpy as np\n","\n","x1_value = np.array(2)\n","x2_value = np.array(3)\n","y_value = evaluator.run(input_dict={x1: x1_value, x2: x2_value})\n","```\n","\n","In this process, the `run` method takes the input values using a dictionary of the form `Dict[Node, numpy.ndarray]`, calculates the value of the node `y` internally, and outputs the result. For instance, with the input values `2 * 3 + 2 = 8`, the expected result for `y_value` would be `np.ndarray(8)`. Note that it will not yield the correct value until you have fully implemented the method:\n","```python\n","np.testing.assert_allclose(y_value, np.array(8))\n","```"]},{"cell_type":"markdown","metadata":{"id":"2LMnlNGTK_h7"},"source":["The `Evaluator.run` method is responsible for the forward computation of nodes. Building on what was discussed in the lectures, to calculate the gradient of the output with respect to each input node within a computational graph, we enhance the forward graph with an additional backward component. By integrating both forward and backward graphs, and providing values for the input nodes, the `Evaluator` can compute the output value, the loss value, and the gradient values for each input node in a single execution of `Evaluator.run`.\n","\n","You are tasked with implementing the function `gradients(output_node: Node, nodes: List[Node]) -> List[Node]` found in `auto_diff.py`. This function constructs the backward graph needed for gradient computation. It accepts an output node—typically the node representing the loss function in machine learning applications, where the gradient is preset to 1. It also takes a list of nodes for which gradients are to be computed and returns a list of gradient nodes corresponding to each node in the input list.\n"]},{"cell_type":"markdown","metadata":{"id":"Sjz5vvUcK_h7"},"source":["Returning to our earlier example, once you have implemented the `gradients` function, you can use it to calculate the gradients of $y$ with respect to $x_1$ and $x_2$. This is done by running:\n","```python\n","x1_grad, x2_grad = ad.gradients(output_node=y, node=[x1, x2])\n","```\n","to obtain the respective gradients. Following this, you can set up an `Evaluator` with nodes `y`, `x1_grad`, and `x2_grad`. This allows you to use the `Evaluator.run` method to compute both the output value and the gradients for the input nodes.\n"]},{"cell_type":"markdown","metadata":{"id":"a9iDUNiYK_h7"},"source":["Before you start working on the assignment, let's clarify how `operations` (ops) work. Within `auto_diff.py`, each op is equipped with three methods:\n","\n","- `__call__(self, **kwargs) -> Node`, which accepts input nodes (and attributes), creates a new node utilizing this op, and returns the newly created node.\n","- `compute(self, node: Node, input_values: List[np.ndarray]) -> np.ndarray`, which processes the specified node along with its input values and delivers the resultant node value.\n","- `gradient(self, node: Node, output_grad: Node) -> List[Node]`, which receives a node and its gradient node, returning the partial adjoint nodes for each input node.\n","\n","In essence, the `Op.compute` method is responsible for calculating the value of an individual node based on its inputs, while the `Evaluator.run` function computes the value of the entire graph's output based on the graph's inputs. The `Op.gradient` method is designed to construct the backward computational graph for an individual node, whereas the `gradients` function builds the backward graph for the entire graph. Accordingly, your implementation of `Evaluator.run` should effectively utilize the `compute` method from op, and your implementation of the `gradients` function should make use of the `gradient` method provided by op.\n"]},{"cell_type":"markdown","metadata":{"id":"moOEW6aNK_h7"},"source":["### Your tasks\n","\n","**Task 1.** \n","Implement the `compute` method for all operations in `auto_diff.py`. We have supplied examples for `AddOp` and `AddByConstOp` to guide you, but you will need to implement the remaining operations. For the scope of this homework, it is safe to assume that the inputs for operations like addition, multiplication, and division will be of the same shape.\n","\n","Sample tests are provided in `tests/test_auto_diff_node_forward.py`. To evaluate your implementation of Task 1, you can execute these tests by running:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1146,"status":"ok","timestamp":1706414004471,"user":{"displayName":"Ruihang Lai","userId":"03605576729730874720"},"user_tz":300},"id":"13URNWeEK_h8","outputId":"a75d39e6-e114-455a-cbb8-44c1fc793ef8"},"outputs":[],"source":["!python3 -m unittest tests/test_auto_diff_node_forward.py"]},{"cell_type":"markdown","metadata":{"id":"-x38qNHDK_h8"},"source":["**Task 2.** \n","Implement the `Executor.run` method in `auto_diff.py`. It may be beneficial to perform a [topological sort](https://en.wikipedia.org/wiki/Topological_sorting) of the computational graph to efficiently compute the output value.\n","\n","Sample tests are available in `tests/test_auto_diff_graph_forward.py`. You can evaluate your implementation of Task 2 by executing these tests:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":939,"status":"ok","timestamp":1706414006440,"user":{"displayName":"Ruihang Lai","userId":"03605576729730874720"},"user_tz":300},"id":"pP92pGx2K_h8","outputId":"9a034a75-19b5-4ef7-af62-1145c245bf6b"},"outputs":[],"source":["!python3 -m unittest tests/test_auto_diff_graph_forward.py"]},{"cell_type":"markdown","metadata":{"id":"rZx-3v2EK_h8"},"source":["**Task 3.** \n","Implement the `gradient` method for all operations in `auto_diff.py`. We have provided examples for `AddOp` and `AddByConstOp` to guide you, but you will need to complete the implementations for the remaining operations.\n","\n","Sample tests are provided in `tests/test_auto_diff_node_backward.py`. To evaluate your implementation of Task 3, you can execute these tests by running:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1376,"status":"ok","timestamp":1706414009774,"user":{"displayName":"Ruihang Lai","userId":"03605576729730874720"},"user_tz":300},"id":"cuFjQYnZK_h8","outputId":"414188a1-695a-4af3-cc68-4d88103fef16"},"outputs":[],"source":["!python3 -m unittest tests/test_auto_diff_node_backward.py"]},{"cell_type":"markdown","metadata":{"id":"C96znygCK_h9"},"source":["**Task 4.** \n","Implement the `gradients` function in `auto_diff.py`. Utilizing a topological sort might prove useful for this implementation.\n","\n","Sample tests are available in `tests/test_auto_diff_graph_backward.py`. You can assess your Task 4 implementation by running these tests:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1190,"status":"ok","timestamp":1706414011383,"user":{"displayName":"Ruihang Lai","userId":"03605576729730874720"},"user_tz":300},"id":"sIPbNc0UK_h9","outputId":"cfd50c77-e18a-4c17-9c1e-d7ea0678ae83"},"outputs":[],"source":["!python3 -m unittest tests/test_auto_diff_graph_backward.py"]},{"cell_type":"markdown","metadata":{},"source":["### A few notes\n","1. **Zero-rank arrays in NumPy.** Throughout this homework, all values are treated as `numpy.ndarray` types. An interesting aspect of NumPy is how it handles zero-rank arrays. For example, adding two zero-rank arrays (`np.array(1) + np.array(2)`) produces a scalar value instead of another zero-rank array:\n","```\n",">>> x = np.array(1)\n",">>> y = np.array(2)\n",">>> type(x), type(y), x.ndim, y.ndim\n","(<class 'numpy.ndarray'>, <class 'numpy.ndarray'>, 0, 0)\n",">>> z = x + y\n",">>> z, type(z)\n","(3, <class 'numpy.int64'>)\n","```\n","For a thorough implementation, you would need to ensure that results are wrapped back into `numpy.ndarray` types. However, for simplicity, this adjustment is optional in this homework. No tests will check for this behavior, and it will not impact your grade. Python does not enforce eager type checking for scalar values.\n","\n","2. **`Node.attrs`.** In our reference implementation of `AddByConstOp` in `auto_diff.py`, the `attrs` field stores the constant operand of the addition. Generally, the `attrs` field holds all **constants** known at the time of constructing the computational graph. For instance, in `AddByConstOp`, the constant operand is stored as a node attribute, whereas in `MatMulOp`, attributes like boolean flags for transposing input matrices are used. You might find it beneficial to store the reduction axis as an attribute when implementing operations like `SumOp`.\n","\n","3. **Minimality of `gradients`.** The `gradients` function builds the backward graph and returns gradient nodes for the specified nodes. It's worth noting that it's not mandatory to construct a minimal backward graph, which would only include necessary gradient nodes. For instance, in the graph `y = x1 * x2 + x1`, if we only require the gradient for `x1 * x2`, a minimal backward graph would exclusively involve this gradient. While this homework doesn't require constructing minimal backward graphs, contemplating the potential advantages or disadvantages of such an approach is a valuable exercise.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ZaOoXdqQyfKV"},"source":["## Part 2: SGD for logistic regression\n","\n","\n","\n","\n","In this section, you are to implement the stochastic gradient descent (SGD) algorithm to train a straightforward logistic regression model.\n","\n","Consider an input vector $x \\in \\mathbb{R}^n$. The logistic regression model we'll use is defined by the equation:\n","$$z = W^T x + b$$\n","Here, $W \\in \\mathbb{R}^{n \\times k}$ represents the weight matrix, $b \\in \\mathbb{R}^k$ the bias vector, and $z \\in \\mathbb{R}^k$ the logits output by the model.\n","\n","The model training will utilize the softmax function combined with cross-entropy loss applied to mini-batches of data. This entails solving the following optimization problem under a mini-batch setting:\n","\\begin{equation}\n","\\min_{W, b} \\;\\; \\ell_{\\mathrm{softmax}}(XW+b, y),\n","\\end{equation}\n","where $X \\in \\mathbb{R}^{b \\times n}$ represents a mini-batch of input data.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"U48mvZZQK_h9"},"source":["### Your tasks\n","\n","In general, you need the following steps (components) to train the logistic regression model:\n","\n","**Task 5.** \n","In the `logistic_regression` function within `logistic_regression.py`, you need to define the forward computational graph for the equation $Z = XW + b$. Here, $XW$ is a 2-dimensional matrix and $b$ is a 1-dimensional vector. This configuration necessitates the introduction of a new operator that can broadcast the vector $b$ to match the matrix dimensions of $XW$. \n","\n","In many frameworks, such as [NumPy](https://numpy.org/doc/stable/reference/generated/numpy.broadcast_to.html), the `broadcast_to` function is used for this purpose. However, since our computational graph nodes do not maintain shape information, you may need to modify the interface of your broadcasting operator to accommodate this. Consider how you can implement this to ensure the vector $b$ correctly aligns with the dimensions of $XW$ within the graph.\n"]},{"cell_type":"markdown","metadata":{"id":"0vBP4Qb3K_h-"},"source":["**Task 6.**\n","Create the `softmax_loss` function in the `logistic_regression.py` file, which builds the necessary computational graph for evaluating softmax loss. This function should receive an input node loaded with logits and another node with one-hot encodings representing true class labels. For cases with multi-class outputs, where $y \\in \\{1, \\ldots, k\\}$, the function utilizes a logits vector $z \\in \\mathbb{R}^k$ and the corresponding true class $y \\in \\{1, \\ldots, k\\}$, which is represented as a one-hot vector. The loss is calculated with the following formula:\n","\n","\\begin{equation}\n","\\ell_{\\mathrm{softmax}}(z, y) = \\log\\sum_{i=1}^k \\exp z_i - z_y.\n","\\end{equation}\n","\n","Additional ops for summing, taking logarithms, and exponentiating may need to be introduced, along with their gradient calculations, to properly construct this softmax loss function.\n"]},{"cell_type":"markdown","metadata":{"id":"VjyrNstkK_h-"},"source":["**Task 7.** \n","Develop the `sgd_epoch` function within `logistic_regression.py` to facilitate a single epoch of stochastic gradient descent (SGD). This function should organize the provided input data and labels into multiple small groups, or mini-batches. Each mini-batch should then be fed sequentially into your pre-constructed computational graph as input.\n","\n","Subsequently, compute the gradients from these operations and proceed to update the weights and biases in your logistic regression model accordingly.\n","\n","Upon successful implementation and execution of this script on a dataset of handwritten digits, you should notice a prediction accuracy approaching 95%. You can verify this by executing the script in your terminal:\n","\n","```shell\n","> python3 train.py\n","...\n","Final test accuracy: 0.9611111111111111\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!python3 train.py"]},{"cell_type":"markdown","metadata":{"id":"E1vY5jDUK_h-"},"source":["**Hint.** When you find the current op set not satisfying your needs, consider introducing a new op."]},{"cell_type":"markdown","metadata":{"id":"83n9F8uYK_h_"},"source":["## How to Submit Your Homework\n","\n","In the home directory for the assignment, execute the command"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!make handin.zip"]},{"cell_type":"markdown","metadata":{},"source":["This will create an archive file with `auto_diff.py` and `logistic_regression.py`. You can check the contents of `handin.zip` to make sure it contains all the needed files:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!unzip handin.zip -d handin"]},{"cell_type":"markdown","metadata":{},"source":["It is expected to list the two files:\n","```\n","auto_diff.py\n","logistic_regression.py\n","```\n","\n","Then, please go to Gradescope and submit the file `handin.zip`.\n","\n","This assignment is automatically graded, and you will receive immediate feedback. You can submit multiple times, but only your final submission will be graded."]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/mlsyscourse/homework1/blob/main/mlsys_hw1.ipynb","timestamp":1706413758241}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
